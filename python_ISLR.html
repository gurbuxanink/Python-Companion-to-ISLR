<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-05-31 Fri 10:40 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>A Python Companion to ISLR</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Naresh Gurbuxani" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">A Python Companion to ISLR</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org103ccbb">1. Introduction</a></li>
<li><a href="#orgc51fd64">2. Statistical Learning</a>
<ul>
<li><a href="#org2acc6ad">2.1. What is Statistical Learning?</a></li>
<li><a href="#org491a505">2.2. Assessing Model Accuracy</a></li>
<li><a href="#org7de6ed0">2.3. Lab: Introduction to Python</a>
<ul>
<li><a href="#orga40f37a">2.3.1. Basic Commands</a></li>
<li><a href="#org9df0107">2.3.2. Graphics</a></li>
<li><a href="#orge5262b9">2.3.3. Indexing Data</a></li>
<li><a href="#org9caf004">2.3.4. Loading Data</a></li>
<li><a href="#orgf2fcd17">2.3.5. Additional Graphical and Numerical Summaries</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1526e73">3. Linear Regression</a>
<ul>
<li><a href="#orgb833fcf">3.1. Simple Linear Regression</a></li>
<li><a href="#orgf9c4e0f">3.2. Multiple Linear Regression</a></li>
<li><a href="#org0371084">3.3. Other Considerations in the Regression Model</a></li>
<li><a href="#org65b0b28">3.4. The Marketing Plan</a></li>
<li><a href="#orga801914">3.5. Comparison of Linear Regression with K-Nearest Neighbors</a></li>
<li><a href="#org7400a8d">3.6. Lab: Linear Regression</a>
<ul>
<li><a href="#org5bb1028">3.6.1. Libraries</a></li>
<li><a href="#orgfc64848">3.6.2. Simple Linear Regression</a></li>
<li><a href="#org2e5908c">3.6.3. Multiple Linear Regression</a></li>
<li><a href="#org7266446">3.6.4. Interaction Terms</a></li>
<li><a href="#org49be44f">3.6.5. Non-linear Transformations of the Predictors</a></li>
<li><a href="#org241ed54">3.6.6. Qualitative Predictors</a></li>
<li><a href="#orge6997aa">3.6.7. Calling <code>R</code> from <code>Python</code></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc96d3eb">4. Classification</a>
<ul>
<li><a href="#orgb5187d5">4.1. An Overview of Classification</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org103ccbb" class="outline-2">
<h2 id="org103ccbb"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Figure <a href="#orgaea3950">1</a> shows graphs of Wage versus three variables. 
</p>


<div id="orgaea3950" class="figure">
<p><img src="figures/fig1_1.png" alt="fig1_1.png" />
</p>
<p><span class="figure-number">Figure 1: </span><code>Wage</code> data, which contains income survey information for males from the central Atlantic region of the United States.  Left: <code>wage</code> as a function of <code>age</code>.  On average, <code>wage</code> increases with <code>age</code> until about 60 years of age, at which point it begins to decline.  Center: <code>wage</code> as a function of <code>year</code>.  There is a slow but steady increase of approximately $10,000 in the average <code>wage</code> between 2003 and 2009.  Right: Boxplots displaying <code>wage</code> as a function of <code>education</code>, with 1 indicating the lowest level (no highschool diploma) and 5 the highest level (an advanced graduate degree).  On average, <code>wage</code> increases with the level of <code>education</code>.</p>
</div>


<p>
Figure <a href="#orgf599b98">2</a> shows boxplots of previous days' percentage changes in S&amp;P
500 grouped according to today's change <code>Up</code> or <code>Down</code>. 
</p>


<div id="orgf599b98" class="figure">
<p><img src="figures/fig1_2.png" alt="fig1_2.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Left: Boxplots of the previous day's percentage change in the S&amp;P 500 index for the days for which the market increased or decreased, obtained from the <code>Smarket</code> data.  Center and Right: Same as left panel, but the percentage changes for two and three days previous are shown.</p>
</div>
</div>
</div>

<div id="outline-container-orgc51fd64" class="outline-2">
<h2 id="orgc51fd64"><span class="section-number-2">2</span> Statistical Learning</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org2acc6ad" class="outline-3">
<h3 id="org2acc6ad"><span class="section-number-3">2.1</span> What is Statistical Learning?</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Figure <a href="#orgbf89786">3</a> shows scatter plots of <code>sales</code> versus <code>TV</code>, <code>radio</code>,
and <code>newspaper</code> advertising.  In each panel, the figure also includes an OLS
regression line.  
</p>


<div id="orgbf89786" class="figure">
<p><img src="figures/fig2_1.png" alt="fig2_1.png" />
</p>
<p><span class="figure-number">Figure 3: </span>The <code>Advertising</code> data set. The plot displays <code>sales</code>, in thousands of units, as a function of <code>TV</code>, <code>radio</code>, and <code>newspaper</code> budgets, in thousands of dollars, for 200 different markets.  In each plot we show the simple least squares fit of <code>sales</code> to that variable.  In other words, each red line represents a simple model that can be used to predict <code>sales</code> using <code>TV</code>, <code>radio</code>, and <code>newspaper</code>, respectively.</p>
</div>


<p>
Figure <a href="#orgf714675">4</a> is a plot of <code>Income</code> versus <code>Years of Education</code> from the
Income data set.  In the left panel, the ``true'' function (given by blue line)
is actually my guess.  
</p>


<div id="orgf714675" class="figure">
<p><img src="figures/fig2_2.png" alt="fig2_2.png" />
</p>
<p><span class="figure-number">Figure 4: </span>The <code>Income</code> data set.  Left: The red dots are the observed values of <code>income</code> (in tens of thousands of dollars) and <code>years of education</code> for 30 individuals.  Right: The blue curve represents the true underlying relationship between <code>income</code> and <code>years of education</code>, which is generally unknown (but is known in this case because the data are simulated).  The vertical lines represent the error associated with each observation.  Note that some of the errors are positive (when an observation lies above the blue curve) and some are negative (when an observation lies below the curve).  Overall, these errors have approximately mean zero.</p>
</div>


<p>
Figure <a href="#orgc74e542">5</a> is a plot of <code>Income</code> versus <code>Years of Education</code> and
<code>Seniority</code> from the <code>Income</code> data set.  Since the book does not provide the
true values of <code>Income</code>, ``true'' values shown in the plot are actually third
order polynomial fit.  
</p>


<div id="orgc74e542" class="figure">
<p><img src="figures/fig2_3.png" alt="fig2_3.png" />
</p>
<p><span class="figure-number">Figure 5: </span>The plot displays <code>income</code> as a function of <code>years of education</code> and <code>seniority</code> in the <code>Income</code> data set.  The blue surface represents the true underlying relationship between <code>income</code> and <code>years of education</code> and <code>seniority</code>, which is known since the data are simulated.  The red dots indicate the observed values of these quantities for 30 individuals.</p>
</div>


<p>
Figure <a href="#orge991180">6</a> shows an example of the parametric approach applied to
the <code>Income</code> data from previous figure. 
</p>


<div id="orge991180" class="figure">
<p><img src="figures/fig2_4.png" alt="fig2_4.png" />
</p>
<p><span class="figure-number">Figure 6: </span>A linear model fit by least squares to the <code>Income</code> data from figure <a href="#orgc74e542">5</a>.  The observations are shown in red, and the blue plane indicates the least squares fit to the data.</p>
</div>


<p>
Figure <a href="#org1593949">7</a> provides an illustration of the trade-off between
flexibility and interpretability for some of the methods covered in this book.
</p>


<div id="org1593949" class="figure">
<p><img src="figures/figure2_7.png" alt="figure2_7.png" />
</p>
<p><span class="figure-number">Figure 7: </span>A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods.  In general, as the flexibility of a method increases, its interpretability decreases.</p>
</div>


<p>
Figure <a href="#org6fc03d4">8</a> provides a simple illustration of the clustering problem.
</p>


<div id="org6fc03d4" class="figure">
<p><img src="figures/fig2_8.png" alt="fig2_8.png" />
</p>
<p><span class="figure-number">Figure 8: </span>A clustering data set involving three groups.  Each group is shown using a different colored symbol.  Left: The three groups are well-separated.  In this setting, a clustering approach should successfully identify the three groups.  Right: There is some overlap among the groups.  Now the clustering taks is more challenging.</p>
</div>
</div>
</div>

<div id="outline-container-org491a505" class="outline-3">
<h3 id="org491a505"><span class="section-number-3">2.2</span> Assessing Model Accuracy</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Figure <a href="#orgb874c94">9</a> illustrates the tradeoff between training MSE and test
MSE.  We select a ``true function'' whose shape is similar to that shown in the
book.  In the left panel, the orange, blue, and green curves illustrate three possible estimates
for \(f\) given by the black curve.  The orange line is the linear regression
fit, which is relatively inflexible.  The blue and green curves were produced
using <i>smoothing splines</i> from <code>UnivariateSpline</code> function in <code>scipy</code> package.
We obtain different levels of flexibility by varying the parameter <code>s</code>, which
affects the number of knots.  
</p>

<p>
For the right panel, we have chosen polynomial fits.  The degree of polynomial
represents the level of flexibility.  This is because the function
<code>UnivariateSpline</code> does not more than five degrees of freedom.  
</p>

<p>
When we repeat the simulations for figure <a href="#orgb874c94">9</a>, we see considerable
variation in the right panel MSE plots.  But the overall conclusion remains the
same.   
</p>


<div id="orgb874c94" class="figure">
<p><img src="figures/fig2_9.png" alt="fig2_9.png" />
</p>
<p><span class="figure-number">Figure 9: </span>Left: Data simulated from \(f\), shown in black.  Three estimates of \(f\) are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves).  Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed grey line).</p>
</div>


<p>
Figure <a href="#org3f854e2">10</a> provides another example in which the true \(f\) is
approximately linear. 
</p>


<div id="org3f854e2" class="figure">
<p><img src="figures/fig2_10.png" alt="fig2_10.png" />
</p>
<p><span class="figure-number">Figure 10: </span>Details are as in figure <a href="#orgb874c94">9</a> using a different true \(f\) that is much closer to linear.  In this setting, linear regression provides a very good fit to the data.</p>
</div>


<p>
Figure <a href="#orgcf42a6b">11</a> displays an example in which \(f\) is highly
non-linear. The training and test MSE curves still exhibit the same general
patterns.
</p>


<div id="orgcf42a6b" class="figure">
<p><img src="figures/fig2_11.png" alt="fig2_11.png" />
</p>
<p><span class="figure-number">Figure 11: </span>Details are as in figure <a href="#orgb874c94">9</a>, using a different \(f\) that is far from linear.  In this setting, linear regression provides a very poor fit to the data.</p>
</div>


<p>
Figure <a href="#orgd2560af">12</a> displays the relationship between bias, variance, and
test MSE.  This relationship is referred to as <i>bias-variance trade-off</i>.  When
simulations are repeated, we see considerable variation in different graphs,
especially for MSE lines.  But overall shape remains the same. 
</p>


<div id="orgd2560af" class="figure">
<p><img src="figures/fig2_12.png" alt="fig2_12.png" />
</p>
<p><span class="figure-number">Figure 12: </span>Squared bias (blue curve), variance (orange curve), \(Var(\epsilon)\) (dashed line), and test MSE (red curve) for the three data sets in figures <a href="#orgb874c94">9</a> - <a href="#orgcf42a6b">11</a>.  The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.</p>
</div>


<p>
Figure <a href="#org2f9c5a9">13</a> provides an example using a simulated data set in
two-dimensional space consisting of predictors \(X_1\) and \(X_2\).  
</p>


<div id="org2f9c5a9" class="figure">
<p><img src="figures/fig2_13.png" alt="fig2_13.png" />
</p>
<p><span class="figure-number">Figure 13: </span>A simulated data set consisting of 200 observations in two groups, indicated in blue and orange.  The dashed line represents the Bayes decision boundary.  The orange background grid indicates the region in which a test observation will be assigned to the orange class, and blue background grid indicates the region in which a test observation will be assigned to the blue class.</p>
</div>


<p>
Figure <a href="#orgf249282">14</a> displays the KNN decision boundary, using \(K=10\), when
applied to the simulated data set from figure <a href="#org2f9c5a9">13</a>.  Even though
the true distribution is not known by the KNN classifier, the KNN decision
making boundary is very close to that of the Bayes classifier.  
</p>


<div id="orgf249282" class="figure">
<p><img src="figures/fig2_15.png" alt="fig2_15.png" />
</p>
<p><span class="figure-number">Figure 14: </span>The firm line indicates the KNN decision boundary on the data from figure <a href="#org2f9c5a9">13</a>, using \(K = 10\). The Bayes decision boundary is shown as a dashed line.  The KNN and Bayes decision boundaries are very similar.</p>
</div>



<div id="org2f55114" class="figure">
<p><img src="figures/fig2_16.png" alt="fig2_16.png" />
</p>
<p><span class="figure-number">Figure 15: </span>A comparison of the KNN decision boundaries (solid curves) obtained using \(K=1\) and \(K=100\) on the data from figure <a href="#org2f9c5a9">13</a>.  With \(K=1\), the decision boundary is overly flexible, while with \(K=100\) it is not sufficiently flexible.  The Bayes decision boundary is shown as dashed line.</p>
</div>


<p>
In figure <a href="#orge0ce82f">16</a> we have plotted the KNN test and training errors as
a function of \(\frac{1}{K}\).  As \(\frac{1}{K}\) increases, the method becomes
more flexible.  As in the regression setting, the training error rate
consistently declines as the flexibility increases.  However, the test error
exhibits the characteristic U-shape, declining at first (with a minimum at
approximately \(K=10\)) before increasing again when the method becomes
excessively flexible and overfits. 
</p>


<div id="orge0ce82f" class="figure">
<p><img src="figures/fig2_17.png" alt="fig2_17.png" />
</p>
<p><span class="figure-number">Figure 16: </span>The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from figure <a href="#org2f9c5a9">13</a> as the level of flexibility (assessed using \(\frac{1}{K}\)) increases, or equivalently as the number of neighbors \(K\) decreases.  The black dashed line indicates the Bayes error rate.</p>
</div>
</div>
</div>

<div id="outline-container-org7de6ed0" class="outline-3">
<h3 id="org7de6ed0"><span class="section-number-3">2.3</span> Lab: Introduction to Python</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orga40f37a" class="outline-4">
<h4 id="orga40f37a"><span class="section-number-4">2.3.1</span> Basic Commands</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
In <code>Python</code> a list can be created by enclosing comma-separated elements by
square brackets.  Length of a list can be obtained using <code>len</code> function.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">x</span> = [1, 3, 2, 5]
<span style="color: #a020f0;">print</span>(<span style="color: #483d8b;">len</span>(x))
<span style="color: #a0522d;">y</span> = 3
<span style="color: #a0522d;">z</span> = 5
<span style="color: #a020f0;">print</span>(y + z)
</pre>
</div>

<pre class="example">
4
8

</pre>

<p>
To create an array of numbers, use <code>array</code> function in <code>numpy</code> library.  <code>numpy</code>
functions can be used to perform element-wise operations on arrays.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a0522d;">x</span> = np.array([[1, 2], [3, 4]])
<span style="color: #a0522d;">y</span> = np.array([6, 7, 8, 9]).reshape((2, 2))
<span style="color: #a020f0;">print</span>(x)
<span style="color: #a020f0;">print</span>(y)
<span style="color: #a020f0;">print</span>(x ** 2)
<span style="color: #a020f0;">print</span>(np.sqrt(y))
</pre>
</div>

<pre class="example">
[[1 2]
 [3 4]]
[[6 7]
 [8 9]]
[[ 1  4]
 [ 9 16]]
[[2.44948974 2.64575131]
 [2.82842712 3.        ]]

</pre>


<p>
<code>numpy.random</code> has a number of functions to generate random variables that
follow a given distribution.  Here we create two correlated sets of numbers, <code>x</code>
and <code>y</code>, and use <code>numpy.corrcoef</code> to calculate correlation between them. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
np.random.seed(911)
<span style="color: #a0522d;">x</span> = np.random.normal(size=50)
<span style="color: #a0522d;">y</span> = x + np.random.normal(loc=50, scale=0.1, size=50)
<span style="color: #a020f0;">print</span>(np.corrcoef(x, y))
<span style="color: #a020f0;">print</span>(np.corrcoef(x, y)[0, 1])
<span style="color: #a020f0;">print</span>(np.mean(x))
<span style="color: #a020f0;">print</span>(np.var(y))
<span style="color: #a020f0;">print</span>(np.std(y) ** 2)
</pre>
</div>

<pre class="example">
[[1.         0.99374931]
 [0.99374931 1.        ]]
0.9937493134584551
-0.020219724397254404
0.9330621750073689
0.9330621750073688

</pre>
</div>
</div>

<div id="outline-container-org9df0107" class="outline-4">
<h4 id="org9df0107"><span class="section-number-4">2.3.2</span> Graphics</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
<code>matplotlib</code> library has a number of functions to plot data in <code>Python</code>.  It is
possible to view graphs on screen or save them in file for inclusion in a
document. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib               <span style="color: #b22222;"># </span><span style="color: #b22222;">only if we need to save figure in file</span>
matplotlib.use(<span style="color: #8b2252;">'Agg'</span>)           <span style="color: #b22222;"># </span><span style="color: #b22222;">only to save figure in file</span>
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #a0522d;">x</span> = np.random.normal(size=100)
<span style="color: #a0522d;">y</span> = np.random.normal(size=100)
plt.plot(x, y)
plt.xlabel(<span style="color: #8b2252;">'This is x-axis'</span>)
plt.ylabel(<span style="color: #8b2252;">'This is y-axis'</span>)
plt.title(<span style="color: #8b2252;">'Plot of X vs Y'</span>)

plt.savefig(<span style="color: #8b2252;">'xyPlot.png'</span>)       <span style="color: #b22222;"># </span><span style="color: #b22222;">only to save figure in a file</span>
</pre>
</div>

<p>
<code>numpy</code> function <code>linspace</code> can be used to create a sequence between a start and
an end of a given length.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #a0522d;">x</span> = np.linspace(-np.pi, np.pi, num=50)
<span style="color: #a0522d;">y</span> = x
<span style="color: #a0522d;">xx</span>, <span style="color: #a0522d;">yy</span> = np.meshgrid(x, y)
<span style="color: #a0522d;">zz</span> = np.cos(yy) / (1 + xx ** 2)

plt.contour(xx, yy, zz)

<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots()
<span style="color: #a0522d;">zza</span> = (zz - zz.T) / 2.0
<span style="color: #a0522d;">CS</span> = ax.contour(xx, yy, zza)
ax.clabel(CS, inline=1)
</pre>
</div>
</div>
</div>

<div id="outline-container-orge5262b9" class="outline-4">
<h4 id="orge5262b9"><span class="section-number-4">2.3.3</span> Indexing Data</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
To access elements of an array, specify indexes inside square brackets.  It is
possible to access multiple rows and columns. <code>shape</code> method gives number of
rows followed by number of columns. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np

<span style="color: #a0522d;">A</span> = np.array(np.arange(1, 17))
<span style="color: #a0522d;">A</span> = A.reshape(4, 4, order=<span style="color: #8b2252;">'F'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">column first, Fortran style</span>
<span style="color: #a020f0;">print</span>(A)
<span style="color: #a020f0;">print</span>(A[1, 2])
<span style="color: #a020f0;">print</span>(A[(0,2),:][:,(1,3)])
<span style="color: #a020f0;">print</span>(A[<span style="color: #483d8b;">range</span>(0,3),:][:,<span style="color: #483d8b;">range</span>(1,4)])
<span style="color: #a020f0;">print</span>(A[<span style="color: #483d8b;">range</span>(0, 2), :])
<span style="color: #a020f0;">print</span>(A[:, <span style="color: #483d8b;">range</span>(0, 2)])
<span style="color: #a020f0;">print</span>(A[0,:])
<span style="color: #a020f0;">print</span>(A.shape)
</pre>
</div>

<pre class="example">
[[ 1  5  9 13]
 [ 2  6 10 14]
 [ 3  7 11 15]
 [ 4  8 12 16]]
10
[ 5 15]
[ 5 10 15]
[[ 1  5  9 13]
 [ 2  6 10 14]]
[[1 5]
 [2 6]
 [3 7]
 [4 8]]
(4, 4)
</pre>
</div>
</div>

<div id="outline-container-org9caf004" class="outline-4">
<h4 id="org9caf004"><span class="section-number-4">2.3.4</span> Loading Data</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
<code>pandas</code> library provides <code>read_csv</code> function to read files with data in
rectangular shape.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd
<span style="color: #a0522d;">Auto</span> = pd.read_csv(<span style="color: #8b2252;">'data/Auto.csv'</span>)
<span style="color: #a020f0;">print</span>(Auto.head())
<span style="color: #a020f0;">print</span>(Auto.shape)
<span style="color: #a020f0;">print</span>(Auto.columns)
</pre>
</div>

<pre class="example">
    mpg  cylinders  displacement  ... year  origin                       name
0  18.0          8         307.0  ...   70       1  chevrolet chevelle malibu
1  15.0          8         350.0  ...   70       1          buick skylark 320
2  18.0          8         318.0  ...   70       1         plymouth satellite
3  16.0          8         304.0  ...   70       1              amc rebel sst
4  17.0          8         302.0  ...   70       1                ford torino

[5 rows x 9 columns]
(397, 9)
Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',
       'acceleration', 'year', 'origin', 'name'],
      dtype='object')
</pre>

<p>
To load data from an <code>R</code> library, use <code>get_rdataset</code> function from
<code>statsmodels</code>.  This function seems to work only if the computer is connected to
the internet. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets
<span style="color: #a0522d;">carseats</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Carseats'</span>, package=<span style="color: #8b2252;">'ISLR'</span>).data
<span style="color: #a020f0;">print</span>(carseats.shape)
<span style="color: #a020f0;">print</span>(carseats.columns)
</pre>
</div>

<pre class="example">
(400, 11)
Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',
       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],
      dtype='object')

</pre>
</div>
</div>

<div id="outline-container-orgf2fcd17" class="outline-4">
<h4 id="orgf2fcd17"><span class="section-number-4">2.3.5</span> Additional Graphical and Numerical Summaries</h4>
<div class="outline-text-4" id="text-2-3-5">
<p>
<code>plot</code> method can be directly applied to a <code>pandas</code> dataframe.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd
<span style="color: #a0522d;">Auto</span> = pd.read_csv(<span style="color: #8b2252;">'data/Auto.csv'</span>)
Auto.boxplot(column=<span style="color: #8b2252;">'mpg'</span>, by=<span style="color: #8b2252;">'cylinders'</span>, grid=<span style="color: #008b8b;">False</span>)
</pre>
</div>

<p>
<code>hist</code> method can be applied to plot a histogram. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd
<span style="color: #a0522d;">Auto</span> = pd.read_csv(<span style="color: #8b2252;">'data/Auto.csv'</span>)
Auto.hist(column=<span style="color: #8b2252;">'mpg'</span>)
Auto.hist(column=<span style="color: #8b2252;">'mpg'</span>, color=<span style="color: #8b2252;">'red'</span>)
Auto.hist(column=<span style="color: #8b2252;">'mpg'</span>, color=<span style="color: #8b2252;">'red'</span>, bins=15)
</pre>
</div>

<p>
For pairs plot, use <code>scatter_matrix</code> method in <code>pandas.plotting</code>.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd
<span style="color: #a020f0;">from</span> pandas <span style="color: #a020f0;">import</span> plotting
<span style="color: #a0522d;">Auto</span> = pd.read_csv(<span style="color: #8b2252;">'data/Auto.csv'</span>)
plotting.scatter_matrix(Auto[[<span style="color: #8b2252;">'mpg'</span>, <span style="color: #8b2252;">'displacement'</span>, <span style="color: #8b2252;">'horsepower'</span>, <span style="color: #8b2252;">'weight'</span>,
                              <span style="color: #8b2252;">'acceleration'</span>]])
</pre>
</div>

<p>
On <code>pandas</code> dataframes, <code>describe</code> method produces a summary of each variable. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd
<span style="color: #a0522d;">Auto</span> = pd.read_csv(<span style="color: #8b2252;">'data/Auto.csv'</span>)
<span style="color: #a020f0;">print</span>(Auto.describe())
</pre>
</div>

<pre class="example">
              mpg   cylinders  ...        year      origin
count  397.000000  397.000000  ...  397.000000  397.000000
mean    23.515869    5.458438  ...   75.994962    1.574307
std      7.825804    1.701577  ...    3.690005    0.802549
min      9.000000    3.000000  ...   70.000000    1.000000
25%     17.500000    4.000000  ...   73.000000    1.000000
50%     23.000000    4.000000  ...   76.000000    1.000000
75%     29.000000    8.000000  ...   79.000000    2.000000
max     46.600000    8.000000  ...   82.000000    3.000000

[8 rows x 7 columns]
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org1526e73" class="outline-2">
<h2 id="org1526e73"><span class="section-number-2">3</span> Linear Regression</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgb833fcf" class="outline-3">
<h3 id="orgb833fcf"><span class="section-number-3">3.1</span> Simple Linear Regression</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Figure <a href="#org8407043">17</a> displays the simple linear regression fit to the
<code>Advertising</code> data, where \(\hat{\beta_0} =\) 0.0475
 and \(\hat{\beta_1} =\) 7.0326.
</p>


<div id="org8407043" class="figure">
<p><img src="figures/fig3_1.png" alt="fig3_1.png" />
</p>
<p><span class="figure-number">Figure 17: </span>For the <code>Advertising</code> data, the least squares fit for the regression of <code>sales</code> onto <code>TV</code> is shown.  The fit is found by minimizing the sum of squared errors.  Each grey line represents an error, and the fit makes a compromise by averaging their squares.  In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot.</p>
</div>




<p>
In figure <a href="#org509dbe9">18</a>, we have computed RSS for a number of values of
\(\beta_0\) and \(\beta_1\), using the advertising data with <code>sales</code> as the response
and <code>TV</code> as the predictor. 
</p>


<div id="org509dbe9" class="figure">
<p><img src="figures/fig3_2.png" alt="fig3_2.png" />
</p>
<p><span class="figure-number">Figure 18: </span>Contour and three-dimensional plots of the RSS on the <code>Advertising</code> data, using <code>sales</code> as the response and <code>TV</code> as the predictor.  The red dots correspond to the least squares estimates \(\hat{\beta_0}\) and \(\hat{\beta_1}\).</p>
</div>


<p>
The left-hand panel of figure <a href="#org9e97be3">19</a> displays <i>population regression
line</i> and <i>least squares line</i> for a simple simulated example.  The red line in
the left-hand panel displays the <i>true</i> relationship, \(f(X) = 2 + 3X\), while the
blue line is the least squares estimate based on observed data.  In the
right-hand panel of figure <a href="#org9e97be3">19</a> we have generated five different
data sets from the model \(Y = 2 + 3X + \epsilon\) and plotted the corresponding
five least squares lines.  
</p>


<div id="org9e97be3" class="figure">
<p><img src="figures/fig3_3.png" alt="fig3_3.png" />
</p>
<p><span class="figure-number">Figure 19: </span>A simulated data set.  Left: The red line represents the true relationship, \(f(X) = 2 + 3X\), which is known as the population regression line.  The blue line is the least squares line; it is the least squares estimate for \(f(X)\) based on the observed data, shown in grey circles.  Right: The population regression line is again shown in red, and the least squares line in blue.  In cyan, five least squares lines are shown, each computed on the basis of a separate random set of observations.  Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.</p>
</div>

<p>
For <code>Advertising</code> data, table <a href="#orge39a23e">1</a> provides details of the least squares model for the
regression of number of units sold on TV advertising budget. 
</p>

<table id="orge39a23e" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> For <code>Advertising</code> data, the coefficients of the least squares model for the regression of number of units sold on TV advertising budget.  An increase of $1,000 on the TV advertising budget is associated with an increase in sales by around 50 units.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">7.0326</td>
<td class="org-right">0.4578</td>
<td class="org-right">15.3603</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">TV</td>
<td class="org-right">0.0475</td>
<td class="org-right">0.0027</td>
<td class="org-right">17.6676</td>
<td class="org-right">0.0</td>
</tr>
</tbody>
</table>


<p>
Next, in table <a href="#orga0b348c">2</a>, we report more information about the least squares model.  
</p>


<table id="orga0b348c" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> For the <code>Advertising</code> data, more information about the least squares model for the regression of number of units sold on TV advertising budget.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Quantity</th>
<th scope="col" class="org-right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Residual standard error</td>
<td class="org-right">3.259</td>
</tr>

<tr>
<td class="org-left">\(R^2\)</td>
<td class="org-right">0.612</td>
</tr>

<tr>
<td class="org-left">F-statistic</td>
<td class="org-right">312.145</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgf9c4e0f" class="outline-3">
<h3 id="orgf9c4e0f"><span class="section-number-3">3.2</span> Multiple Linear Regression</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Table <a href="#orgf4f4bf3">3</a>  shows results of two simple linear
regressions, each of which uses a different advertising medium as a predictor.
We find that a $1,000 increase in spending on radio advertising is associated
with an increase in sales by around 202 units.  A $1,000 increase in advertising
spending on on newspapers increases sales by approximately 55 units. 
</p>

<table id="orgf4f4bf3" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> More simple linear regression models for <code>Advertising</code> data.  Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget and Bottom: newspaper advertising budget. A $1,000 increase in spending on radio advertising is associated with an average increase sales by around 202 units, while the same increase in spending on newspaper advertising is associated with an average increase of around 55 units.  <code>Sales</code> variable is in thousands of units, and the <code>radio</code> and <code>newspaper</code> variables are in thousands of dollars..</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">9.312</td>
<td class="org-right">0.563</td>
<td class="org-right">16.542</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">radio</td>
<td class="org-right">0.202</td>
<td class="org-right">0.02</td>
<td class="org-right">9.921</td>
<td class="org-right">0.0</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">12.351</td>
<td class="org-right">0.621</td>
<td class="org-right">19.876</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">newspaper</td>
<td class="org-right">0.055</td>
<td class="org-right">0.017</td>
<td class="org-right">3.3</td>
<td class="org-right">0.001</td>
</tr>
</tbody>
</table>


<p>
Figure <a href="#org0d289f9">20</a> illustrates an example of the least squares fit to a
toy data set with \(p = 2\) predictors. 
</p>


<div id="org0d289f9" class="figure">
<p><img src="figures/fig3_4.png" alt="fig3_4.png" />
</p>
<p><span class="figure-number">Figure 20: </span>In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane.  The plane is chosen to minimize the sum of the squared vertical distances between each observation (shown in red) and the plane.</p>
</div>


<p>
Table <a href="#org9d19e6e">4</a> displays multiple regression coefficient estimates when
TV, radio, and newspaper advertising budgets are used to predict product sales
using <code>Advertising</code> data.
</p>

<table id="org9d19e6e" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 4:</span> For the <code>Advertising</code> data, least squares coefficient estimates of the multiple linear regression of number of units sold on radio, TV, and newspaper advertising budgets.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">2.939</td>
<td class="org-right">0.312</td>
<td class="org-right">9.422</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">TV</td>
<td class="org-right">0.046</td>
<td class="org-right">0.001</td>
<td class="org-right">32.809</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">radio</td>
<td class="org-right">0.189</td>
<td class="org-right">0.009</td>
<td class="org-right">21.893</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">newspaper</td>
<td class="org-right">-0.001</td>
<td class="org-right">0.006</td>
<td class="org-right">-0.177</td>
<td class="org-right">0.86</td>
</tr>
</tbody>
</table>

<p>
Table <a href="#org6439798">5</a> shows the correlation matrix for the three predictor
variables and response variable in table <a href="#org9d19e6e">4</a>. 
</p>

<table id="org6439798" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 5:</span> Correlation matrix for <code>TV</code>, <code>radio</code>, and <code>sales</code> for the <code>Advertising</code> data.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">TV</th>
<th scope="col" class="org-right">radio</th>
<th scope="col" class="org-right">newspaper</th>
<th scope="col" class="org-right">sales</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">TV</td>
<td class="org-right">1.0</td>
<td class="org-right">0.0548</td>
<td class="org-right">0.0566</td>
<td class="org-right">0.7822</td>
</tr>

<tr>
<td class="org-left">radio</td>
<td class="org-right">0.0548</td>
<td class="org-right">1.0</td>
<td class="org-right">0.3541</td>
<td class="org-right">0.5762</td>
</tr>

<tr>
<td class="org-left">newspaper</td>
<td class="org-right">0.0566</td>
<td class="org-right">0.3541</td>
<td class="org-right">1.0</td>
<td class="org-right">0.2283</td>
</tr>

<tr>
<td class="org-left">sales</td>
<td class="org-right">0.7822</td>
<td class="org-right">0.5762</td>
<td class="org-right">0.2283</td>
<td class="org-right">1.0</td>
</tr>
</tbody>
</table>

<table id="org5ba0eaa" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 6:</span> More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the <code>Advertising</code> data.  Other information about this model was displayed in table <a href="#org9d19e6e">4</a>.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Quantity</th>
<th scope="col" class="org-right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Residual standard error</td>
<td class="org-right">1.69</td>
</tr>

<tr>
<td class="org-left">\(R^2\)</td>
<td class="org-right">0.897</td>
</tr>

<tr>
<td class="org-left">F-statistic</td>
<td class="org-right">570.0</td>
</tr>
</tbody>
</table>

<p>
Figure <a href="#orgeddbc18">21</a> displays a three-dimensional plot of <code>TV</code> and <code>radio</code>
versus <code>sales</code>.  
</p>


<div id="orgeddbc18" class="figure">
<p><img src="figures/fig3_5.png" alt="fig3_5.png" />
</p>
<p><span class="figure-number">Figure 21: </span>For the <code>Advertising</code> data, a linear regression fit to <code>sales</code> using <code>TV</code> and <code>radio</code> as predictors.  From the pattern of the residuals, we can see that there is a pronounced non-linear relationship in the data.  The positive residuals tend to lie along the 45-degree line, where TV and Radio budgets are split evenly.  The negative residuals tend to lie away from this line, where budgets are more lopsided.</p>
</div>
</div>
</div>

<div id="outline-container-org0371084" class="outline-3">
<h3 id="org0371084"><span class="section-number-3">3.3</span> Other Considerations in the Regression Model</h3>
<div class="outline-text-3" id="text-3-3">
<p>
<code>Credit</code> data set displayed in figure <a href="#org4428c8b">22</a> records <code>balance</code>
(average credit card debt for a number of individuals) as well as several
quantitative predictors: <code>age</code>, <code>cards</code> (number of credit cards), <code>education</code>
and <code>rating</code> (credit rating).
</p>


<div id="org4428c8b" class="figure">
<p><img src="figures/fig3_6.png" alt="fig3_6.png" />
</p>
<p><span class="figure-number">Figure 22: </span>The <code>Credit</code> dataset contains information about <code>balance</code>, <code>age</code>, <code>cards</code>, <code>education</code>, <code>income</code>, <code>limit</code>, and <code>rating</code> for a number of potential customers.</p>
</div>


<p>
Table <a href="#org4a898b5">7</a> displays the coefficient estimates and other information
associated with the model where <code>gender</code> is the only explanatory variable.
</p>

<table id="org4a898b5" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 7:</span> Least squares coefficient estimates associated with the regression of <code>balance</code> onto <code>gender</code> in the <code>Credit</code> data set.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">509.803</td>
<td class="org-right">33.128</td>
<td class="org-right">15.389</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Gender[T.Female]</td>
<td class="org-right">19.733</td>
<td class="org-right">46.051</td>
<td class="org-right">0.429</td>
<td class="org-right">0.669</td>
</tr>
</tbody>
</table>


<p>
From table <a href="#orga716a8a">8</a> we see that the estimated <code>balance</code> for the
baseline, African American, is $531.0. It is estimated that the
Asian category will have an additional $-18.7 debt, and that the
Caucasian category will have an additional $-12.5 debt compared to
Africna American category.
</p>

<table id="orga716a8a" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 8:</span> Least squares coefficient estimates associated with the regression of <code>balance</code> onto <code>ethnicity</code> in the <code>Credit</code> data set.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">531.0</td>
<td class="org-right">46.319</td>
<td class="org-right">11.464</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Ethnicity[T.Asian]</td>
<td class="org-right">-18.686</td>
<td class="org-right">65.021</td>
<td class="org-right">-0.287</td>
<td class="org-right">0.774</td>
</tr>

<tr>
<td class="org-left">Ethnicity[T.Caucasian]</td>
<td class="org-right">-12.503</td>
<td class="org-right">56.681</td>
<td class="org-right">-0.221</td>
<td class="org-right">0.826</td>
</tr>
</tbody>
</table>



<p>
Table <a href="#org8826c63">9</a> shows results of regressing <code>sales</code> and <code>TV</code> and <code>radio</code>
when an interaction term is included.  Coefficient of interaction term
<code>TV:radio</code> is highly significant.
</p>


<p>
In figure <a href="#org90274e5">23</a>, the left panel shows least squares lines when
we predict <code>balance</code> using <code>income</code> (quantitative) and <code>student</code> (qualitative
variables). There is no interaction term between <code>income</code> and <code>student</code>.  The
right panel shows least squares lines when an interaction term is included. 
</p>

<table id="org8826c63" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 9:</span> For <code>Advertising</code> data, least squares coefficient estimates associated with the regression of <code>sales</code> onto <code>TV</code> and <code>radio</code>, with an interaction term.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">6.75</td>
<td class="org-right">0.248</td>
<td class="org-right">27.233</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">TV</td>
<td class="org-right">0.019</td>
<td class="org-right">0.002</td>
<td class="org-right">12.699</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">radio</td>
<td class="org-right">0.029</td>
<td class="org-right">0.009</td>
<td class="org-right">3.241</td>
<td class="org-right">0.001</td>
</tr>

<tr>
<td class="org-left">TV:radio</td>
<td class="org-right">0.001</td>
<td class="org-right">0.0</td>
<td class="org-right">20.727</td>
<td class="org-right">0.0</td>
</tr>
</tbody>
</table>



<div id="org90274e5" class="figure">
<p><img src="figures/fig3_7.png" alt="fig3_7.png" />
</p>
<p><span class="figure-number">Figure 23: </span>For the <code>Credit</code> data, the least squares lines are shown for prediction of <code>balance</code> from <code>income</code> for students and non-students.  Left: There is no interaction between <code>income</code> and <code>student</code>.  Right: There is an interaction term between <code>income</code> and <code>students</code>.</p>
</div>

<p>
Figure <a href="#org80d765e">24</a> shows a scatter plot of <code>mpg</code> (gas mileage in miles per
gallon) versus <code>horsepower</code> in the <code>Auto</code> data set.  The figure also includes
least squares fit line for linear, second degree, and fifth degree polynomials
in <code>horsepower</code>. 
</p>


<div id="org80d765e" class="figure">
<p><img src="figures/fig3_8.png" alt="fig3_8.png" />
</p>
<p><span class="figure-number">Figure 24: </span>The <code>Auto</code> data set.  For a number of cars, <code>mpg</code> and <code>horsepower</code> are shown.  The linear regression fit is shown in orange.  The linear regression fit for a model that includes first- and second-order terms of <code>horsepower</code> is shown as blue curve.  The linear regression fit for a model that includes all polynomials of <code>horsepower</code> up to fifth-degree is shown in green.</p>
</div>


<p>
Table <a href="#orgd252d67">10</a> shows regression results of a qudratic fit to explain
<code>mpg</code> as a function of <code>horsepower</code> and \(\mathttt{horsepower^2}\).  
</p>


<table id="orgd252d67" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 10:</span> For the <code>Auto</code> data set, least squares coefficient estimates associated with the regression of <code>mpg</code> onto <code>horsepower</code> and \(\texttt{horsepower^2}\).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">56.9001</td>
<td class="org-right">1.8004</td>
<td class="org-right">31.6037</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">horsepower</td>
<td class="org-right">-0.4662</td>
<td class="org-right">0.0311</td>
<td class="org-right">-14.9782</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">\(horsepower^2\)</td>
<td class="org-right">0.0012</td>
<td class="org-right">0.0001</td>
<td class="org-right">10.0801</td>
<td class="org-right">0.0</td>
</tr>
</tbody>
</table>


<p>
The left panel of figure <a href="#org72348c2">25</a> displays a residual plot from the
linear regression of <code>mpg</code> onto <code>horsepower</code> on the <code>Auto</code> data set.  The red
line is a smooth fit to the residuals, which is displayed in order to make it
easier to identify any trends.  The residuals exhibit a clear U-shape, which
strongly suggests non-linearity in the data.  In contrast, the right hand panel
of figure<a href="#org72348c2">25</a> displays the residual plot results from the model
which contains a quadratic term in <code>horsepower</code>.  Now there is little pattern in
residuals, suggesting that the quadratic term improves the fit to the data.
</p>


<div id="org72348c2" class="figure">
<p><img src="figures/fig3_9.png" alt="fig3_9.png" />
</p>
<p><span class="figure-number">Figure 25: </span>Plots of residuals versus predicted (or fitted) values for the <code>Auto</code> data set.  In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend.  Left: A linear regression of <code>mpg</code> on <code>horsepower</code>.  A strong pattern in the residuals indicates non-linearity in the data.  Right: A linear regression of <code>mpg</code> on <code>horsepower</code> and square of <code>horsepower</code>.  Now there is little pattern in the residuals.</p>
</div>


<p>
Figure <a href="#org60cfcf9">26</a> provides an illustration of correlations among
residuals.  In the top panel, we see the residuals from a linear regression fit
to data generated with uncorrelated errors.  There is no evidence of
time-related trend in the residuals.  In contrast, the residuals in the bottom
panel are from a data set in which adjacent errors had a correlation of 0.9.
Now there is a clear pattern in the residuals - adjacent residuals tend to take
on similar values.  Finally, the center panel illustrates a more moderate case
in which the residuals had a correlation of 0.5.  There is still evidence of
tracking, but the pattern is less pronounced. 
</p>


<div id="org60cfcf9" class="figure">
<p><img src="figures/fig3_10.png" alt="fig3_10.png" />
</p>
<p><span class="figure-number">Figure 26: </span>Plots of residuals from simulated time series data sets generated with differeing levels of correlation \(\rho\) between error terms for adjacent time points.</p>
</div>


<p>
In the left-hand panel of figure <a href="#orga06e8ad">27</a>, the magnitude of the
residuals tends to increase with the fitted values.  The right hand panel
displays residual plot after tranforming the response using \(\log(Y)\).  The
residuals now appear to have constant variance, although there is some evidence
of a non-linear relationship in the data.
</p>


<div id="orga06e8ad" class="figure">
<p><img src="figures/fig3_11.png" alt="fig3_11.png" />
</p>
<p><span class="figure-number">Figure 27: </span>Residual plots.  The red line, a smooth fit to the residuals, is intended to make it easier to identify a trend.  The blue lines track \(5^{th}\) and \(95^{th}\) percentiles of the residuals, and emphasize patterns.  Left: The funnel shape indicates heteroscedasticity.  Right: the response has been log transformed, and now there is no evidence of heteroscedasticity.</p>
</div>

<p>
The red point (observation 20) in the left hand panel of figure
<a href="#org3479420">28</a> illustrates a typical outlier.  The red solid line is the
least squares regression fit, while the blue dashed line is the least squares
fit after removal of the outlier.  In this case, removal of outlier has little
effect on the least squares line.  In the center panel of figure
<a href="#org3479420">28</a>, the outlier is clearly visible.  In practice, to decide if
the outlier is sufficiently big to be considered an outlier, we can plot
<i>studentized residuals</i>, computed by dividing each residual \(\epsilon_i\) by its
estimated standard error.  These are shown in the right hand panel. 
</p>


<div id="org3479420" class="figure">
<p><img src="figures/fig3_12.png" alt="fig3_12.png" />
</p>
<p><span class="figure-number">Figure 28: </span>Left: The least squares regression line is shown in red.  The regression line after removing the outlier is is shown in blue.  Center: The residual plot clearly identifies the outlier.  Right: The outlier has a studentized residual of 6; typically we expect values between -3 and 3.</p>
</div>

<p>
Observation 41 in the left-hand panel in figure <a href="#org45c36c0">29</a> has
high leverage, in that the predictor value for this observation is large
relative to the other observations.  The data displayed in figure
<a href="#org45c36c0">29</a> are the same as the data displayed in figure
<a href="#org3479420">28</a>, except for the addition of a single high leverage
observation<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>.  The red solid line is the least squares fit to the data,
while the blue dashed line is the fit produced when observation 41 is
removed. Comparing the left-hand panels of figures <a href="#org3479420">28</a> and
<a href="#org45c36c0">29</a>, we observe that removign the high leverage observation has a
much more substantial impact on least squares line than removing the outlier.
The center panel of figure <a href="#org45c36c0">29</a>, for a data set with two
predictors \(X_1\) and \(X_2\). While most of the observations' predictor values
fall within the region of blue dashed lines, the red observation is well outside this
range. But neither the value for \(X_1\) nor the value for \(X_2\) is unusual.  So
if we examine just \(X_1\) or \(X_2\), we will not notice this high leverage
point. The right-panel of figure <a href="#org45c36c0">29</a> provides a plot of
studentized residuals versus \(h_i\) for the data in the left hand panel.
Observation 41 stands out as having a very high leverage statistic as well as a
high studentized residual.
</p>



<div id="org45c36c0" class="figure">
<p><img src="figures/fig3_13.png" alt="fig3_13.png" />
</p>
<p><span class="figure-number">Figure 29: </span>Left: Observation 41 is a high leverage point, while 20 is not.  The red line is the fit to all the data, and the blue line is the fit with observation 41 removed.  Center: The red observation is not unusual in terms of its \(X_1\) value or its \(X_2\) value, but still falls outside the bulk of the data, and hence has high leverage.  Right: Observation 41 has a high leverage and a high residual.</p>
</div>

<p>
Figure <a href="#org8824c98">30</a> illustrates the concept of collinearity.
</p>


<div id="org8824c98" class="figure">
<p><img src="figures/fig3_14.png" alt="fig3_14.png" />
</p>
<p><span class="figure-number">Figure 30: </span>Scatterplots of the observations from the <code>Credit</code> data set.  Left: A plot of <code>age</code> versus <code>limit</code>.  These two variables not collinear.  Right: A plot of <code>rating</code> versus <code>limit</code>.  There is high collinearity.</p>
</div>


<p>
Figure <a href="#org0e11356">31</a> illustrates some of the difficulties that can result
from collinearity.  The left panel is a contour plot of the RSS associated with
different possible coefficient estimates for the regression of <code>balance</code> on
<code>limit</code> and <code>age</code>.  Each ellipse represents a set of coefficients that
correspond to the same RSS, with ellipses nearest to the center taking on the
lowest values of RSS.  The black dot and the associated dashed lines represent
the coefficient estimates that result in the smallest possible RSS.  The axes
for <code>limit</code> and <code>age</code> have been scaled so that the plot includes possible
coefficients that are upto four standard errors on either side of the least
squares estimates.  We see that the true <code>limit</code> coefficient is almost certainly
between 0.15 and 0.20.
</p>

<p>
In contrast, the right hand panel of figure <a href="#org0e11356">31</a> displays contour
plots of the RSS associated with possible coefficient estimates for the
regression of <code>balance</code> onto <code>limit</code> and <code>rating</code>, which we know to be highly
collinear.  Now the contours run along a narrow valley; there is a broad range
of values for the coefficient estimates that result in equal values for RSS.  
</p>


<div id="org0e11356" class="figure">
<p><img src="figures/fig3_15.png" alt="fig3_15.png" />
</p>
<p><span class="figure-number">Figure 31: </span>Contour plots for the RSS values as a function of the parameters \(\beta\) for various regressions involving the <code>Credit</code> data set.  In each plot, the black dots represent the coefficient values corresponding to the minimum RSS.  Left: A contour plot of RSS for the regression of <code>balance</code> onto <code>age</code> and <code>limit</code>.  The minimum value is well defined.  Right: A contour plot of RSS for the regression of <code>balance</code> onto <code>rating</code> and <code>limit</code>.  Because of the collinearity, there are many pairs \((\beta_{Limit}, \beta_{Rating})\) with a similar value for RSS.</p>
</div>

<p>
Table <a href="#org09fcdf7">11</a> compares the coefficient estimates obtained from two
separate multiple regression models.  The first is a regression of <code>balance</code> on
<code>age</code> and <code>limit</code>.  The second is a regression of <code>balance</code> on <code>rating</code> and
<code>limit</code>.  In the first regression, both <code>age</code> and <code>limit</code> are highly significant
with very small p-values.  In the second, the collinearity between <code>limit</code> and
<code>rating</code> has caused the standard error for the <code>limit</code> coefficient to increase
by a factor of 12 and the p-value to increase to 0.701. In other words, the
importance of the <code>limit</code> variable has been masked due to the presence of
collinearity.  
</p>

<table id="org09fcdf7" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 11:</span> The results for two multiple regression models involving the <code>Credit</code> data set.  The top panel is a regression of <code>balance</code> on <code>age</code> and <code>limit</code>.  The bottom panel is a regression of <code>balance</code> on <code>rating</code> and <code>limit</code>.  The standard error of \(\hat{\beta}_{Limit}\) increases 12-fold in the second regression, due to collinearity.</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Coef.</th>
<th scope="col" class="org-right">Std.Err.</th>
<th scope="col" class="org-right">\(t\)</th>
<th scope="col" class="org-right">\(P > \mid t \mid\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">-173.411</td>
<td class="org-right">43.828</td>
<td class="org-right">-3.957</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Age</td>
<td class="org-right">-2.291</td>
<td class="org-right">0.672</td>
<td class="org-right">-3.407</td>
<td class="org-right">0.001</td>
</tr>

<tr>
<td class="org-left">Limit</td>
<td class="org-right">0.173</td>
<td class="org-right">0.005</td>
<td class="org-right">34.496</td>
<td class="org-right">0.0</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Intercept</td>
<td class="org-right">-377.537</td>
<td class="org-right">45.254</td>
<td class="org-right">-8.343</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Rating</td>
<td class="org-right">2.202</td>
<td class="org-right">0.952</td>
<td class="org-right">2.312</td>
<td class="org-right">0.021</td>
</tr>

<tr>
<td class="org-left">Limit</td>
<td class="org-right">0.025</td>
<td class="org-right">0.064</td>
<td class="org-right">0.384</td>
<td class="org-right">0.701</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org65b0b28" class="outline-3">
<h3 id="org65b0b28"><span class="section-number-3">3.4</span> The Marketing Plan</h3>
</div>

<div id="outline-container-orga801914" class="outline-3">
<h3 id="orga801914"><span class="section-number-3">3.5</span> Comparison of Linear Regression with K-Nearest Neighbors</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Figure <a href="#orgf630064">32</a> illustrates two KNN fits on a data set with \(p = 2\)
predictors. The fit with \(K = 1\) is shown in the left-hand panel, while the
right-hand panel displays the fit with \(K = 9\).  When \(K = 1\), the KNN fit
perfectly interpolates the training observations, and consequently takes the
form of a step function. When \(K = 9\), the KNN fit is still a step function, but
averaging over nine observations results in much smaller regions of constant
prediction, and consequently a smoother fit.  
</p>


<div id="orgf630064" class="figure">
<p><img src="figures/fig3_16.png" alt="fig3_16.png" />
</p>
<p><span class="figure-number">Figure 32: </span>Plots of \(\hat{f}(X)\) using KNN regression on two-dimensional data set with 64 observations (brown dots).  Left: \(K = 1\) results in a rough step function fit.  Right: \(K = 9\) produces a much smoother fit.</p>
</div>

<p>
Figure <a href="#org720b0a9">33</a> provides an example of KNN regression with data
generated from a one-dimensional regression model.  the black dashed lines
represent \(f(X)\), while the blue curves correspond to the KNN fits using \(K = 1\)
and \(K = 9\).  In this case, the \(K = 1\) predictions are far too variable, while
the smoother \(K = 9\) fit is much closer to \(f(X)\).  
</p>


<div id="org720b0a9" class="figure">
<p><img src="figures/fig3_17.png" alt="fig3_17.png" />
</p>
<p><span class="figure-number">Figure 33: </span>Plots of \(\hat{f}(X)\) using KNN regression on a one-dimensional data set with 50 observations.  The true relationship is given by the black dashed line.  Left: The blue curve corresponds to \(K = 1\) and interpolates (i.e., passes directly through) training data.  Right: The blue curve corresponds to \(K = 9\), and represents a smoother fit.</p>
</div>

<p>
Figure <a href="#org391011c">34</a> represents the linear regression fit to the same
data.  It is almost perfect.  The right hand panel of figure <a href="#org391011c">34</a>
reveals that linear regression outperforms KNN for this data.  The green line,
plotted as a function of \(\frac{1}{K}\), represents the test set mean squared
error (MSE) for KNN.  The KNN errors are well above the horizontal dashed line,
which is the test MSE for linear regression.
</p>


<div id="org391011c" class="figure">
<p><img src="figures/fig3_18.png" alt="fig3_18.png" />
</p>
<p><span class="figure-number">Figure 34: </span>The same data set shown in figure <a href="#org720b0a9">33</a> is investigated further.  Left: The blue dashed line is the least squares fit to the data.  Since \(f(X)\) is in fact linear (displayed in black line), the least squares regression line provides a very good estimate of \(f(X)\).  Right: The dashed horizontal line represents the least squares test set MSE, while the green line corresponds to the MSE for KNN as a function of \(\frac{1}{K}\).  Linear regression achieves a lower test MSE than does KNN regression, since \(f(X)\) is in fact linear.</p>
</div>


<p>
Figure <a href="#org482f347">35</a> examines the relative performances of least squares
regression and KNN under increasing levels of non-linearity in the relationship
between \(X\) and \(Y\).  In the top row, the true relationship is nearly linear.
In this case, we see that the test MSE for linear regression is still superior
to that of KNN for low values of \(K\) (far right).  However, as \(K\) increases,
KNN outperforms linear regression.  The second row illustrates a more
substantial deviation from linearity.  In this situation, KNN substantially
outperforms linear regression for all values of \(K\).  
</p>


<div id="org482f347" class="figure">
<p><img src="figures/fig3_19.png" alt="fig3_19.png" />
</p>
<p><span class="figure-number">Figure 35: </span>Top Left: In a setting with a slightly non-linear relationship between \(X\) and \(Y\) (solid black line), the KNN fits with \(K = 1\) (blue) and \(K = 9\) (red) are displayed.  Top Right: For the slightly non-linear data,the test set MSE for least squares regression (horizontal) and KNN with various values of \(\frac{1}{K}\) (green) are displayed.  Bottom Left and Bottom Right: As in the top panel, but with a strongly non-linear relationship between \(X\) and \(Y\).</p>
</div>


<p>
Figure <a href="#org92a6b1b">36</a> considers the same strongly non-linear situation as in the lower
panel of figure <a href="#org482f347">35</a>, except that we have added additional <i>noise</i>
predictors that are not associated with the response.  When \(p = 1\) or \(p = 2\),
KNN outperforms linear regression.  But as we increase \(p\), linear regression
becomes superior to KNN.  In fact, increase in dimensionality has only caused a
small increase in linear regression test set MSE, but it has caused a much
bigger increase in the MSE for KNN.
</p>


<div id="org92a6b1b" class="figure">
<p><img src="figures/fig3_20.png" alt="fig3_20.png" />
</p>
<p><span class="figure-number">Figure 36: </span>Test MSE for linear regressions (black horizontal lines) and KNN (green curves) as the number of variables \(p\) increases.  The true function is non-linear in the first variable, as in the lower panel in figure <a href="#org482f347">35</a>, and does not depend upon the additional variables. The performance of linear regression deteriorates slowly in the presense of these additional variables, whereas KNN's performance degrades more quickly as \(p\) increases.</p>
</div>
</div>
</div>

<div id="outline-container-org7400a8d" class="outline-3">
<h3 id="org7400a8d"><span class="section-number-3">3.6</span> Lab: Linear Regression</h3>
<div class="outline-text-3" id="text-3-6">
</div>
<div id="outline-container-org5bb1028" class="outline-4">
<h4 id="org5bb1028"><span class="section-number-4">3.6.1</span> Libraries</h4>
<div class="outline-text-4" id="text-3-6-1">
<p>
The <code>import</code> function, along with an optional <code>as</code>, is used to load <i>libraries</i>.
Before a library can be loaded, it must be installed on the system. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
</pre>
</div>
</div>
</div>

<div id="outline-container-orgfc64848" class="outline-4">
<h4 id="orgfc64848"><span class="section-number-4">3.6.2</span> Simple Linear Regression</h4>
<div class="outline-text-4" id="text-3-6-2">
<p>
We load <code>Boston</code> data set from <code>R</code> library <code>MASS</code>.  Then we use <code>ols</code> function
from <code>statsmodels.formula.api</code> to fit simple linear regression model, with
<code>medv</code> as response and <code>lstat</code> as the predictor.
</p>

<p>
Function <code>summary2()</code> gives some basic information about the model.  We can use
<code>dir()</code> to find out what other pieces of information are stored in <code>lm_fit</code>.
The <code>predict()</code> function can be used to produce prediction of <code>medv</code> for a given
value of <code>lstat</code>. 
</p>

<div class="org-src-container">
<pre class="src src-python" id="org29f6d60"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data
<span style="color: #a020f0;">print</span>(boston.columns)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">lm_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat'</span>, data=boston)
<span style="color: #a0522d;">lm_fit</span> = lm_reg.fit()
<span style="color: #a020f0;">print</span>(lm_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(<span style="color: #483d8b;">dir</span>(lm_fit))
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(lm_fit.predict(exog=<span style="color: #483d8b;">dict</span>(lstat=[5, 10, 15])))
</pre>
</div>

<pre class="example">
Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',
       'ptratio', 'black', 'lstat', 'medv'],
      dtype='object')
--------
                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.543    
Dependent Variable: medv             AIC:                3286.9750
Date:               2019-05-28 14:10 BIC:                3295.4280
No. Observations:   506              Log-Likelihood:     -1641.5  
Df Model:           1                F-statistic:        601.6    
Df Residuals:       504              Prob (F-statistic): 5.08e-88 
R-squared:          0.544            Scale:              38.636   
-------------------------------------------------------------------
               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]
-------------------------------------------------------------------
Intercept     34.5538    0.5626   61.4151  0.0000  33.4485  35.6592
lstat         -0.9500    0.0387  -24.5279  0.0000  -1.0261  -0.8740
------------------------------------------------------------------
Omnibus:             137.043       Durbin-Watson:          0.892  
Prob(Omnibus):       0.000         Jarque-Bera (JB):       291.373
Skew:                1.453         Prob(JB):               0.000  
Kurtosis:            5.319         Condition No.:          30     
==================================================================

------
['HC0_se', 'HC1_se', 'HC2_se', 'HC3_se', '_HCCM', '__class__', '__delattr__', 
'__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', 
'__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', 
'__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', 
'__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', 
'__subclasshook__', '__weakref__', '_cache', '_data_attr', 
'_get_robustcov_results', '_is_nested', '_wexog_singular_values', 'aic', 
'bic', 'bse', 'centered_tss', 'compare_f_test', 'compare_lm_test', 
'compare_lr_test', 'condition_number', 'conf_int', 'conf_int_el', 'cov_HC0', 
'cov_HC1', 'cov_HC2', 'cov_HC3', 'cov_kwds', 'cov_params', 'cov_type', 
'df_model', 'df_resid', 'eigenvals', 'el_test', 'ess', 'f_pvalue', 'f_test', 
'fittedvalues', 'fvalue', 'get_influence', 'get_prediction', 
'get_robustcov_results', 'initialize', 'k_constant', 'llf', 'load', 'model', 
'mse_model', 'mse_resid', 'mse_total', 'nobs', 'normalized_cov_params', 
'outlier_test', 'params', 'predict', 'pvalues', 'remove_data', 'resid', 
'resid_pearson', 'rsquared', 'rsquared_adj', 'save', 'scale', 'ssr', 
'summary', 'summary2', 't_test', 't_test_pairwise', 'tvalues', 
'uncentered_tss', 'use_t', 'wald_test', 'wald_test_terms', 'wresid']
------
0    29.803594
1    25.053347
2    20.303101
dtype: float64
</pre>

<p>
We will now plot <code>medv</code> and <code>lstat</code> along with least squares regression line.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data
<span style="color: #a020f0;">print</span>(boston.columns)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">lm_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat'</span>, data=boston)
<span style="color: #a0522d;">lm_fit</span> = lm_reg.fit()
<span style="color: #a020f0;">print</span>(lm_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(<span style="color: #483d8b;">dir</span>(lm_fit))
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(lm_fit.predict(exog=<span style="color: #483d8b;">dict</span>(lstat=[5, 10, 15])))
<span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #a0522d;">fig</span> = plt.figure()
<span style="color: #a0522d;">ax</span> = fig.add_subplot(111)
boston.plot(x=<span style="color: #8b2252;">'lstat'</span>, y=<span style="color: #8b2252;">'medv'</span>, alpha=0.7, ax=ax)
sm.graphics.abline_plot(model_results=lm_fit, ax=ax, c=<span style="color: #8b2252;">'r'</span>)

</pre>
</div>

<p>
Next we examine some diagnostic plots.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data
<span style="color: #a020f0;">print</span>(boston.columns)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">lm_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat'</span>, data=boston)
<span style="color: #a0522d;">lm_fit</span> = lm_reg.fit()
<span style="color: #a020f0;">print</span>(lm_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(<span style="color: #483d8b;">dir</span>(lm_fit))
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'------'</span>)

<span style="color: #a020f0;">print</span>(lm_fit.predict(exog=<span style="color: #483d8b;">dict</span>(lstat=[5, 10, 15])))
<span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a020f0;">from</span> statsmodels.nonparametric.smoothers_lowess <span style="color: #a020f0;">import</span> lowess
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np

<span style="color: #a0522d;">fig</span> = plt.figure()
<span style="color: #a0522d;">ax1</span> = fig.add_subplot(221)
ax1.scatter(lm_fit.fittedvalues, lm_fit.resid, s=5, c=<span style="color: #8b2252;">'b'</span>, alpha=0.6)
ax1.axhline(y=0, linestyle=<span style="color: #8b2252;">'--'</span>, c=<span style="color: #8b2252;">'r'</span>)
<span style="color: #b22222;"># </span><span style="color: #b22222;">resid_lowess_fit = lowess(endog=lm_fit.resid, exog=lm_fit.fittedvalues,</span>
<span style="color: #b22222;">#                           </span><span style="color: #b22222;">is_sorted=True)</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">ax1.plot(resid_lowess_fit[:,0], resid_lowess_fit[:,1]) </span>
ax1.set_xlabel(<span style="color: #8b2252;">'Fitted values'</span>)
ax1.set_ylabel(<span style="color: #8b2252;">'Residuals'</span>)
ax1.set_title(<span style="color: #8b2252;">'Residuals vs Fitted'</span>)

<span style="color: #a0522d;">ax2</span>=fig.add_subplot(222)
sm.graphics.qqplot(lm_fit.resid, ax=ax2, markersize=3, line=<span style="color: #8b2252;">'s'</span>,
                   linestyle=<span style="color: #8b2252;">'--'</span>, fit=<span style="color: #008b8b;">True</span>, alpha=0.4)
ax2.set_ylabel(<span style="color: #8b2252;">'Standardized residuals'</span>)
ax2.set_title(<span style="color: #8b2252;">'Normal Q-Q'</span>)

<span style="color: #a0522d;">influence</span> = lm_fit.get_influence()
<span style="color: #a0522d;">standardized_resid</span> = influence.resid_studentized_internal
<span style="color: #a0522d;">ax3</span> = fig.add_subplot(223)
ax3.scatter(lm_fit.fittedvalues, np.sqrt(np.<span style="color: #483d8b;">abs</span>(standardized_resid)), s=5,
            alpha=0.4, c=<span style="color: #8b2252;">'b'</span>)
ax3.set_xlabel(<span style="color: #8b2252;">'Fitted values'</span>)
ax3.set_ylabel(r<span style="color: #8b2252;">'$\sqrt{\mid Standardized\; residuals \mid}$'</span>)
ax3.set_title(<span style="color: #8b2252;">'Scale-Location'</span>)

<span style="color: #a0522d;">ax4</span> = fig.add_subplot(224)
sm.graphics.influence_plot(lm_fit, size=2, alpha=0.4, c=<span style="color: #8b2252;">'b'</span>,  ax=ax4)
ax4.xaxis.label.set_size(10)
ax4.yaxis.label.set_size(10)
ax4.title.set_size(12)
ax4.set_xlim(0, 0.03)
<span style="color: #a020f0;">for</span> txt <span style="color: #a020f0;">in</span> ax4.texts:
    txt.set_visible(<span style="color: #008b8b;">False</span>)
ax4.axhline(y=0, linestyle=<span style="color: #8b2252;">'--'</span>, color=<span style="color: #8b2252;">'grey'</span>)

fig.tight_layout()
</pre>
</div>
</div>
</div>

<div id="outline-container-org2e5908c" class="outline-4">
<h4 id="org2e5908c"><span class="section-number-4">3.6.3</span> Multiple Linear Regression</h4>
<div class="outline-text-4" id="text-3-6-3">
<p>
In order to fit a multiple regression model using least squares, we again use
the <code>ols</code> and <code>fit</code> functions.  The syntax <code>ols(formula</code>'y ~ x1 + x2 + x3') is
used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>.  The
<code>summary2()</code> now outputs the regression coefficients for all three predictors. 
</p>

<p>
<code>statsmodels</code> does not seem to have <code>R</code> like facility to include all variables
using the formula <code>y ~ .</code>.  To include all variables, we either write them
individually, or use code to create a formula.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data

<span style="color: #a0522d;">lm_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat + age'</span>, data=boston)
<span style="color: #a0522d;">lm_fit</span> = lm_reg.fit()

<span style="color: #a020f0;">print</span>(lm_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Create formula to include all variables</span>
<span style="color: #a0522d;">all_columns</span> = <span style="color: #483d8b;">list</span>(boston.columns)
all_columns.remove(<span style="color: #8b2252;">'medv'</span>)
<span style="color: #a0522d;">my_formula</span> = <span style="color: #8b2252;">'medv ~ '</span> + <span style="color: #8b2252;">' + '</span>.join(all_columns)
<span style="color: #a020f0;">print</span>(my_formula)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">all_reg</span> = smf.ols(formula=my_formula, data=boston)
<span style="color: #a0522d;">all_fit</span> = all_reg.fit()
<span style="color: #a020f0;">print</span>(all_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)
</pre>
</div>

<pre class="example">
                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.549    
Dependent Variable: medv             AIC:                3281.0064
Date:               2019-05-29 10:07 BIC:                3293.6860
No. Observations:   506              Log-Likelihood:     -1637.5  
Df Model:           2                F-statistic:        309.0    
Df Residuals:       503              Prob (F-statistic): 2.98e-88 
R-squared:          0.551            Scale:              38.108   
-------------------------------------------------------------------
               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]
-------------------------------------------------------------------
Intercept     33.2228    0.7308   45.4579  0.0000  31.7869  34.6586
lstat         -1.0321    0.0482  -21.4163  0.0000  -1.1267  -0.9374
age            0.0345    0.0122    2.8256  0.0049   0.0105   0.0586
------------------------------------------------------------------
Omnibus:             124.288       Durbin-Watson:          0.945  
Prob(Omnibus):       0.000         Jarque-Bera (JB):       244.026
Skew:                1.362         Prob(JB):               0.000  
Kurtosis:            5.038         Condition No.:          201    
==================================================================

--------
medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + 
ptratio + black + lstat
--------
                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.734    
Dependent Variable: medv             AIC:                3025.6086
Date:               2019-05-29 10:07 BIC:                3084.7801
No. Observations:   506              Log-Likelihood:     -1498.8  
Df Model:           13               F-statistic:        108.1    
Df Residuals:       492              Prob (F-statistic): 6.72e-135
R-squared:          0.741            Scale:              22.518   
-------------------------------------------------------------------
            Coef.    Std.Err.     t      P&gt;|t|    [0.025    0.975] 
-------------------------------------------------------------------
Intercept   36.4595    5.1035    7.1441  0.0000   26.4322   46.4868
crim        -0.1080    0.0329   -3.2865  0.0011   -0.1726   -0.0434
zn           0.0464    0.0137    3.3816  0.0008    0.0194    0.0734
indus        0.0206    0.0615    0.3343  0.7383   -0.1003    0.1414
chas         2.6867    0.8616    3.1184  0.0019    0.9939    4.3796
nox        -17.7666    3.8197   -4.6513  0.0000  -25.2716  -10.2616
rm           3.8099    0.4179    9.1161  0.0000    2.9887    4.6310
age          0.0007    0.0132    0.0524  0.9582   -0.0253    0.0266
dis         -1.4756    0.1995   -7.3980  0.0000   -1.8675   -1.0837
rad          0.3060    0.0663    4.6129  0.0000    0.1757    0.4364
tax         -0.0123    0.0038   -3.2800  0.0011   -0.0197   -0.0049
ptratio     -0.9527    0.1308   -7.2825  0.0000   -1.2098   -0.6957
black        0.0093    0.0027    3.4668  0.0006    0.0040    0.0146
lstat       -0.5248    0.0507  -10.3471  0.0000   -0.6244   -0.4251
------------------------------------------------------------------
Omnibus:             178.041       Durbin-Watson:          1.078  
Prob(Omnibus):       0.000         Jarque-Bera (JB):       783.126
Skew:                1.521         Prob(JB):               0.000  
Kurtosis:            8.281         Condition No.:          15114  
==================================================================
* The condition number is large (2e+04). This might indicate
strong multicollinearity or other numerical problems.
--------
</pre>
</div>
</div>

<div id="outline-container-org7266446" class="outline-4">
<h4 id="org7266446"><span class="section-number-4">3.6.4</span> Interaction Terms</h4>
<div class="outline-text-4" id="text-3-6-4">
<p>
The syntax <code>lstat:black</code> tells <code>ols</code> to include an interaction term between
<code>lstat</code> and <code>black</code>.  The syntax <code>lstat*age</code> simultaneously includes <code>lstat,
age,</code> and the intraction term \(\text{lstat} \times \text{age]\) as predictors.
It is a shorthand for <code>lstat + age + lstat:age</code>. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data

<span style="color: #a0522d;">my_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat * age'</span>, data=boston)
<span style="color: #a0522d;">my_fit</span> = my_reg.fit()
<span style="color: #a020f0;">print</span>(my_fit.summary2())
</pre>
</div>

<pre class="example">
                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.553    
Dependent Variable: medv             AIC:                3277.9547
Date:               2019-05-29 11:48 BIC:                3294.8609
No. Observations:   506              Log-Likelihood:     -1635.0  
Df Model:           3                F-statistic:        209.3    
Df Residuals:       502              Prob (F-statistic): 4.86e-88 
R-squared:          0.556            Scale:              37.804   
-------------------------------------------------------------------
                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]
-------------------------------------------------------------------
Intercept      36.0885    1.4698  24.5528  0.0000  33.2007  38.9763
lstat          -1.3921    0.1675  -8.3134  0.0000  -1.7211  -1.0631
age            -0.0007    0.0199  -0.0363  0.9711  -0.0398   0.0383
lstat:age       0.0042    0.0019   2.2443  0.0252   0.0005   0.0078
------------------------------------------------------------------
Omnibus:             135.601       Durbin-Watson:          0.965  
Prob(Omnibus):       0.000         Jarque-Bera (JB):       296.955
Skew:                1.417         Prob(JB):               0.000  
Kurtosis:            5.461         Condition No.:          6878   
==================================================================
* The condition number is large (7e+03). This might indicate
strong multicollinearity or other numerical problems.
</pre>
</div>
</div>

<div id="outline-container-org49be44f" class="outline-4">
<h4 id="org49be44f"><span class="section-number-4">3.6.5</span> Non-linear Transformations of the Predictors</h4>
<div class="outline-text-4" id="text-3-6-5">
<p>
The <code>ols</code> function can also accomodate non-linear transformations of the
predictors.  For example, given a predictor \(X\), we can create predictor \(X^2\)
using <code>I(X ** 2)</code>.  We now perform a regression of <code>medv</code> onto <code>lstat</code> and
\(\texttt{lstat}^2\). 
</p>

<p>
The near-zero p-value associated with the quadratic term suggests that it leads
to an improve model.  We use <code>anova_lm()</code> function to further quantify the
extent to which the quadratic fit is superior to the linear fit.  The null
hypothesis is that the two models fit the data equally well.  The alternative
hypothesis is that the full model is superior.  Given the large F-statistic and
zero p-value, this provides very clear evidence that the model with quadratic
term is superior.  A plot of residuals versus fitted values shows that, with
quadratic term included, there is no discernible pattern in residuals. 
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets
<span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a0522d;">lowess</span> = sm.nonparametric.lowess
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #a0522d;">boston</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Boston'</span>, <span style="color: #8b2252;">'MASS'</span>).data

<span style="color: #a0522d;">my_reg</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat'</span>, data=boston)
<span style="color: #a0522d;">my_fit</span> = my_reg.fit()

<span style="color: #a0522d;">my_reg2</span> = smf.ols(formula=<span style="color: #8b2252;">'medv ~ lstat + I(lstat ** 2)'</span>, data=boston)
<span style="color: #a0522d;">my_fit2</span> = my_reg2.fit()
<span style="color: #a020f0;">print</span>(my_fit.summary2())
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a020f0;">print</span>(sm.stats.anova_lm(my_fit2))
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a020f0;">print</span>(sm.stats.anova_lm(my_fit, my_fit2))

<span style="color: #a0522d;">my_regs</span> = (my_reg, my_reg2)

<span style="color: #a0522d;">fig</span> = plt.figure(figsize=(8,4))
<span style="color: #a0522d;">i_reg</span> = 1
<span style="color: #a020f0;">for</span> reg <span style="color: #a020f0;">in</span> my_regs:
    <span style="color: #a0522d;">ax</span> = fig.add_subplot(1, 2, i_reg)
    <span style="color: #a0522d;">fit</span> = reg.fit()
    ax.scatter(fit.fittedvalues, fit.resid, s=7, alpha=0.6)
    <span style="color: #a0522d;">lowess_fit</span> = lowess(fit.resid, fit.fittedvalues)
    ax.plot(lowess_fit[:,0], lowess_fit[:,1], c=<span style="color: #8b2252;">'r'</span>)
    ax.axhline(y=0, linestyle=<span style="color: #8b2252;">'--'</span>, color=<span style="color: #8b2252;">'grey'</span>)
    ax.set_xlabel(<span style="color: #8b2252;">'Fitted values'</span>)
    ax.set_ylabel(<span style="color: #8b2252;">'Residuals'</span>)
    ax.set_title(reg.formula)
    <span style="color: #a0522d;">i_reg</span> += 1

fig.tight_layout()
</pre>
</div>

<pre class="example">
                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.543    
Dependent Variable: medv             AIC:                3286.9750
Date:               2019-05-29 12:41 BIC:                3295.4280
No. Observations:   506              Log-Likelihood:     -1641.5  
Df Model:           1                F-statistic:        601.6    
Df Residuals:       504              Prob (F-statistic): 5.08e-88 
R-squared:          0.544            Scale:              38.636   
-------------------------------------------------------------------
               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]
-------------------------------------------------------------------
Intercept     34.5538    0.5626   61.4151  0.0000  33.4485  35.6592
lstat         -0.9500    0.0387  -24.5279  0.0000  -1.0261  -0.8740
------------------------------------------------------------------
Omnibus:             137.043       Durbin-Watson:          0.892  
Prob(Omnibus):       0.000         Jarque-Bera (JB):       291.373
Skew:                1.453         Prob(JB):               0.000  
Kurtosis:            5.319         Condition No.:          30     
==================================================================

--------
                  df        sum_sq       mean_sq           F         PR(&gt;F)
lstat            1.0  23243.913997  23243.913997  761.810354  8.819026e-103
I(lstat ** 2)    1.0   4125.138260   4125.138260  135.199822   7.630116e-28
Residual       503.0  15347.243158     30.511418         NaN            NaN
--------
   df_resid           ssr  df_diff     ss_diff           F        Pr(&gt;F)
0     504.0  19472.381418      0.0         NaN         NaN           NaN
1     503.0  15347.243158      1.0  4125.13826  135.199822  7.630116e-28
</pre>
</div>
</div>

<div id="outline-container-org241ed54" class="outline-4">
<h4 id="org241ed54"><span class="section-number-4">3.6.6</span> Qualitative Predictors</h4>
<div class="outline-text-4" id="text-3-6-6">
<p>
We will now examine <code>Carseats</code> data, which is part of the <code>ISLR</code> library.  We
will attempt to predict <code>Sales</code> (child car seat sales) based on a number of
predictors. <code>statsmodels</code> automatically converts string variables into
categorical variables.  If we want <code>statsmodels</code> to treat a numerical variable <code>x</code> as
qualitative predictor, the formula should be <code>y ~ C(x)</code>. Here <code>C()</code> stands for
categorical.  
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.formula.api <span style="color: #a020f0;">as</span> smf
<span style="color: #a020f0;">from</span> statsmodels <span style="color: #a020f0;">import</span> datasets

<span style="color: #a0522d;">carseats</span> = datasets.get_rdataset(<span style="color: #8b2252;">'Carseats'</span>, <span style="color: #8b2252;">'ISLR'</span>).data
<span style="color: #a020f0;">print</span>(carseats.columns)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">all_columns</span> = <span style="color: #483d8b;">list</span>(carseats.columns)
all_columns.remove(<span style="color: #8b2252;">'Sales'</span>)
<span style="color: #a0522d;">my_formula</span> = <span style="color: #8b2252;">'Sales ~ '</span> + <span style="color: #8b2252;">' + '</span>.join(all_columns)
<span style="color: #a0522d;">my_formula</span> +=  <span style="color: #8b2252;">' + Income:Advertising + Price:Age'</span>

<span style="color: #a020f0;">print</span>(my_formula)
<span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">'--------'</span>)

<span style="color: #a0522d;">my_reg</span> = smf.ols(formula=my_formula, data=carseats)
<span style="color: #a0522d;">my_fit</span> = my_reg.fit()
<span style="color: #a020f0;">print</span>(my_fit.summary2())
</pre>
</div>

<pre class="example">
Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',
       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],
      dtype='object')
--------
Sales ~ CompPrice + Income + Advertising + Population + Price + ShelveLoc + Age + Education + Urban + US + Income:Advertising + Price:Age
--------
                  Results: Ordinary least squares
====================================================================
Model:                OLS              Adj. R-squared:     0.872    
Dependent Variable:   Sales            AIC:                1157.3378
Date:                 2019-05-29 12:53 BIC:                1213.2183
No. Observations:     400              Log-Likelihood:     -564.67  
Df Model:             13               F-statistic:        210.0    
Df Residuals:         386              Prob (F-statistic): 6.14e-166
R-squared:            0.876            Scale:              1.0213   
--------------------------------------------------------------------
                     Coef.  Std.Err.    t     P&gt;|t|   [0.025  0.975]
--------------------------------------------------------------------
Intercept            6.5756   1.0087   6.5185 0.0000  4.5922  8.5589
ShelveLoc[T.Good]    4.8487   0.1528  31.7243 0.0000  4.5482  5.1492
ShelveLoc[T.Medium]  1.9533   0.1258  15.5307 0.0000  1.7060  2.2005
Urban[T.Yes]         0.1402   0.1124   1.2470 0.2132 -0.0808  0.3612
US[T.Yes]           -0.1576   0.1489  -1.0580 0.2907 -0.4504  0.1352
CompPrice            0.0929   0.0041  22.5668 0.0000  0.0848  0.1010
Income               0.0109   0.0026   4.1828 0.0000  0.0058  0.0160
Advertising          0.0702   0.0226   3.1070 0.0020  0.0258  0.1147
Population           0.0002   0.0004   0.4329 0.6653 -0.0006  0.0009
Price               -0.1008   0.0074 -13.5494 0.0000 -0.1154 -0.0862
Age                 -0.0579   0.0160  -3.6329 0.0003 -0.0893 -0.0266
Education           -0.0209   0.0196  -1.0632 0.2884 -0.0594  0.0177
Income:Advertising   0.0008   0.0003   2.6976 0.0073  0.0002  0.0013
Price:Age            0.0001   0.0001   0.8007 0.4238 -0.0002  0.0004
--------------------------------------------------------------------
Omnibus:                1.281        Durbin-Watson:           2.047 
Prob(Omnibus):          0.527        Jarque-Bera (JB):        1.147 
Skew:                   0.129        Prob(JB):                0.564 
Kurtosis:               3.050        Condition No.:           130576
====================================================================
* The condition number is large (1e+05). This might indicate
strong multicollinearity or other numerical problems.
</pre>
</div>
</div>

<div id="outline-container-orge6997aa" class="outline-4">
<h4 id="orge6997aa"><span class="section-number-4">3.6.7</span> Calling <code>R</code> from <code>Python</code></h4>
</div>
</div>
</div>


<div id="outline-container-orgc96d3eb" class="outline-2">
<h2 id="orgc96d3eb"><span class="section-number-2">4</span> Classification</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgb5187d5" class="outline-3">
<h3 id="orgb5187d5"><span class="section-number-3">4.1</span> An Overview of Classification</h3>
<div class="outline-text-3" id="text-4-1">
<p>
In figure <a href="#org7d425b7">37</a>, we have plotted annual <code>income</code> and monthly
credit card <code>balance</code> for a subset of individuals in <code>Credit</code> data set.  The
left hand panel displays individuals who defaulted in brown, and those who did
not in blue.  We have plotted only a fraction of individuals who did not
default.  It appears that individuals who defaulted tended to have higher credit
card balances than those who did not.  In the right hand panel, we show two
pairs of boxplots.  The first shows the distribution of <code>balance</code> split by the
binary <code>default</code> variable; the second is a similar plot for <code>income</code>.  
</p>


<div id="org7d425b7" class="figure">
<p><img src="figures/fig4_1.png" alt="fig4_1.png" />
</p>
<p><span class="figure-number">Figure 37: </span>The <code>Default</code> data set.  Left: The annual income and monthly credit card balances of a number of individuals.  The individuals who defaulted on their credit card debt are shown in brown, and those who did not default are shown in blue.  Center: Boxplots of <code>balance</code> as a function of <code>default</code> status.  Right: Boxplots of <code>income</code> as a function of <code>default</code> status.</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
The middle panel is from a different data set.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Naresh Gurbuxani</p>
<p class="date">Created: 2019-05-31 Fri 10:40</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
