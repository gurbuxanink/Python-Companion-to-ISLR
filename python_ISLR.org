#+TITLE: Python Companion to ISLR
#+LATEX_HEADER: \usepackage{placeins}
#+LATEX_HEADER: \hypersetup{colorlinks=true, allcolors=blue, linkbordercolor=white}

* Introduction

Figure [[fig:introFig1]] shows graphs of Wage versus three variables. 

#+NAME: fig1_1plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig1_1.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap1/')

  import wagePlot
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:introFig1
#+CAPTION: =Wage= data, which contains income survey information for males from the central Atlantic region of the United States.  Left: =wage= as a function of =age=.  On average, =wage= increases with =age= until about 60 years of age, at which point it begins to decline.  Center: =wage= as a function of =year=.  There is a slow but steady increase of approximately $10,000 in the average =wage= between 2003 and 2009.  Right: Boxplots displaying =wage= as a function of =education=, with 1 indicating the lowest level (no highschool diploma) and 5 the highest level (an advanced graduate degree).  On average, =wage= increases with the level of =education=.
#+RESULTS: fig1_1plot
[[file:figures/fig1_1.png]]


Figure [[fig:introFig2]] shows boxplots of previous days' percentage changes in S&P
500 grouped according to today's change =Up= or =Down=. 

#+NAME: fig1_2plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig1_2.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap1/')

  import SmarketPlot
  plt.savefig(fname)
  return fname
#+END_SRC


#+NAME: fig:introFig2
#+CAPTION: Left: Boxplots of the previous day's percentage change in the S&P 500 index for the days for which the market increased or decreased, obtained from the =Smarket= data.  Center and Right: Same as left panel, but the percentage changes for two and three days previous are shown.
#+RESULTS: fig1_2plot
[[file:figures/fig1_2.png]]

#+LATEX: \FloatBarrier

* Statistical Learning

** What is Statistical Learning?

Figure [[fig:statLearnFig1]] shows scatter plots of =sales= versus =TV=, =radio=,
and =newspaper= advertising.  In each panel, the figure also includes an OLS
regression line.  

#+NAME: fig2_1plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_1.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap2/')

  import salesPlot
  plt.savefig(fname)
  return fname
#+END_SRC


#+NAME: fig:statLearnFig1
#+CAPTION: The =Advertising= data set. The plot displays =sales=, in thousands of units, as a function of =TV=, =radio=, and =newspaper= budgets, in thousands of dollars, for 200 different markets.  In each plot we show the simple least squares fit of =sales= to that variable.  In other words, each red line represents a simple model that can be used to predict =sales= using =TV=, =radio=, and =newspaper=, respectively.
#+RESULTS: fig2_1plot
[[file:figures/fig2_1.png]]


Figure [[fig:statLearnFig2]] is a plot of =Income= versus =Years of Education= from the
Income data set.  In the left panel, the ``true'' function (given by blue line)
is actually my guess.  

#+NAME: fig2_2plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_2.png"
  import matplotlib
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2/')

  import incomeEdPlot
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig2
#+CAPTION: The =Income= data set.  Left: The red dots are the observed values of =income= (in tens of thousands of dollars) and =years of education= for 30 individuals.  Right: The blue curve represents the true underlying relationship between =income= and =years of education=, which is generally unknown (but is known in this case because the data are simulated).  The vertical lines represent the error associated with each observation.  Note that some of the errors are positive (when an observation lies above the blue curve) and some are negative (when an observation lies below the curve).  Overall, these errors have approximately mean zero.
#+RESULTS: fig2_2plot
[[file:figures/fig2_2.png]]


Figure [[fig:statLearnFig3]] is a plot of =Income= versus =Years of Education= and
=Seniority= from the =Income= data set.  Since the book does not provide the
true values of =Income=, ``true'' values shown in the plot are actually third
order polynomial fit.  

#+NAME: fig2_3plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_3.png"
  import matplotlib
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2')
  import incEdSen3d

  my_formula = 'Income ~ Education + I(Education**2) + I(Education**3) + Seniority + I(Seniority**2) + I(Seniority**3)'

  incEdSen3d.plotIncomeEdSeniority(my_formula)
  plt.savefig(fname)
  return fname
#+END_SRC


#+NAME: fig:statLearnFig3
#+CAPTION: The plot displays =income= as a function of =years of education= and =seniority= in the =Income= data set.  The blue surface represents the true underlying relationship between =income= and =years of education= and =seniority=, which is known since the data are simulated.  The red dots indicate the observed values of these quantities for 30 individuals.
#+RESULTS: fig2_3plot
[[file:figures/fig2_3.png]]


Figure [[fig:statLearnFig4]] shows an example of the parametric approach applied to
the =Income= data from previous figure. 

#+NAME: fig2_4plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_4.png"
  import matplotlib
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2')

  import incomeEdSeniority2_4
  plt.savefig(fname)
  return fname
#+END_SRC


#+NAME: fig:statLearnFig4
#+CAPTION: A linear model fit by least squares to the =Income= data from figure [[fig:statLearnFig3]].  The observations are shown in red, and the blue plane indicates the least squares fit to the data.
#+RESULTS: fig2_4plot
[[file:figures/fig2_4.png]]


Figure [[fig:statLearnFig7]] provides an illustration of the trade-off between
flexibility and interpretability for some of the methods covered in this book.

#+NAME: fig2_7plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/figure2_7.png"
  import matplotlib
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2/')

  import interpretVsFlexibility
  plt.savefig(fname)
  return(fname)
#+END_SRC

#+NAME: fig:statLearnFig7
#+CAPTION: A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods.  In general, as the flexibility of a method increases, its interpretability decreases.
#+RESULTS: fig2_7plot
[[file:figures/figure2_7.png]]


Figure [[fig:statLearnFig8]] provides a simple illustration of the clustering problem.

#+NAME: fig2_8plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_8.png"
  import matplotlib
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2/')

  import clusteringPlot
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig8
#+CAPTION: A clustering data set involving three groups.  Each group is shown using a different colored symbol.  Left: The three groups are well-separated.  In this setting, a clustering approach should successfully identify the three groups.  Right: There is some overlap among the groups.  Now the clustering taks is more challenging.
#+RESULTS: fig2_8plot
[[file:figures/fig2_8.png]]

** Assessing Model Accuracy

Figure [[fig:statLearnFig9]] illustrates the tradeoff between training MSE and test
MSE.  We select a ``true function'' whose shape is similar to that shown in the
book.  In the left panel, the orange, blue, and green curves illustrate three possible estimates
for $f$ given by the black curve.  The orange line is the linear regression
fit, which is relatively inflexible.  The blue and green curves were produced
using /smoothing splines/ from =UnivariateSpline= function in =scipy= package.
We obtain different levels of flexibility by varying the parameter =s=, which
affects the number of knots.  

For the right panel, we have chosen polynomial fits.  The degree of polynomial
represents the level of flexibility.  This is because the function
=UnivariateSpline= does not more than five degrees of freedom.  

When we repeat the simulations for figure [[fig:statLearnFig9]], we see considerable
variation in the right panel MSE plots.  But the overall conclusion remains the
same.   

#+NAME: fig2_9plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_9.png"
  import matplotlib
  import numpy as np
  matplotlib.use('Agg')
  import sys
  import matplotlib.pyplot as plt
  sys.path.append('./code/chap2/')
  import biasVarTradeoff

  # This function shape is similar to book figure 2.9 left panel
  def myFunc(x):
      return 6.0 + 2 * np.sin((x - 40.0) / 20.0)

  biasVarTradeoff.mseVsFlexibility(myFunc)
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig9
#+CAPTION: Left: Data simulated from $f$, shown in black.  Three estimates of $f$ are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves).  Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed grey line). 
#+RESULTS: fig2_9plot
[[file:figures/fig2_9.png]]


Figure [[fig:statLearnFig10]] provides another example in which the true $f$ is
approximately linear. 

#+NAME: fig2_10plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_10.png"
  import matplotlib
  matplotlib.use('Agg')
  import numpy as np
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap2/')
  import biasVarTradeoff

  # This function has shape similar to book figure 2.10
  def myFunc(x):
      return 2 + 0.05 * x + 0.0005 * (x ** 2)

  biasVarTradeoff.mseVsFlexibility(myFunc)
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig10
#+CAPTION: Details are as in figure [[fig:statLearnFig9]] using a different true $f$ that is much closer to linear.  In this setting, linear regression provides a very good fit to the data.
#+RESULTS: fig2_10plot
[[file:figures/fig2_10.png]]


Figure [[fig:statLearnFig11]] displays an example in which $f$ is highly
non-linear. The training and test MSE curves still exhibit the same general
patterns.

#+NAME: fig2_11plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_11.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import numpy as np
  import sys
  sys.path.append('./code/chap2/')
  import biasVarTradeoff

  # This function has shape similar to book figure 2.11
  def myFunc(x):
      return 0.012 * ((x - 35) ** 2) - 0.00025 * ((x - 35) ** 3)

  biasVarTradeoff.mseVsFlexibility(myFunc)
  plt.savefig(fname)
  return fname
#+END_SRC


#+NAME: fig:statLearnFig11
#+CAPTION: Details are as in figure [[fig:statLearnFig9]], using a different $f$ that is far from linear.  In this setting, linear regression provides a very poor fit to the data. 
#+RESULTS: fig2_11plot
[[file:figures/fig2_11.png]]


Figure [[fig:statLearnFig12]] displays the relationship between bias, variance, and
test MSE.  This relationship is referred to as /bias-variance trade-off/.  When
simulations are repeated, we see considerable variation in different graphs,
especially for MSE lines.  But overall shape remains the same. 

#+NAME: fig2_12plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_12.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import numpy as np
  import sys
  sys.path.append('./code/chap2')
  import biasVarError

  # Use same functions which were used to generate figures 2.9, 2.10, and 2.11
  def myFunc(x):
      return 6.0 + 2 * np.sin((x - 40.0) / 20.0)

  degree, mse, var, bias = biasVarError.mseBiasVar(myFunc)
  fig = plt.figure()
  ax1 = fig.add_subplot(131)
  ax1.plot(degree, mse, color='brown', alpha=0.7)
  ax1.plot(degree, bias, color='blue', linestyle='-.')
  ax1.plot(degree, var, color='orange', linestyle='--', alpha=0.9)
  ax1.set_ylim(0, min(5, max(mse)))
  ax1.axhline(y=1, color='grey', linestyle='--')
  min_mse_ind = mse.index(min(mse))
  ax1.axvline(x=degree[min_mse_ind], color='grey', linestyle=':')
  ax1.set_xlabel('Flexibility')


  def myFunc(x):
      return 2 + 0.05 * x + 0.0005 * (x ** 2)

  degree, mse, var, bias = biasVarError.mseBiasVar(myFunc)
  ax2 = fig.add_subplot(132)
  ax2.plot(degree, mse, color='brown', alpha=0.7)
  ax2.plot(degree, bias, color='blue', linestyle='-.')
  ax2.plot(degree, var, color='orange', linestyle='--', alpha=0.9)
  ax2.set_ylim(0, min(5, max(mse)))
  ax2.axhline(y=1, color='grey', linestyle='--')
  min_mse_ind = mse.index(min(mse))
  ax2.axvline(x=degree[min_mse_ind], color='grey', linestyle=':')
  ax2.set_xlabel('Flexibility')


  def myFunc(x):
      return 0.012 * ((x - 35) ** 2) - 0.00025 * ((x - 35) ** 3)

  degree, mse, var, bias = biasVarError.mseBiasVar(myFunc)
  ax3 = fig.add_subplot(133)
  ax3.plot(degree, mse, color='brown', alpha=0.7, label='MSE')
  ax3.plot(degree, bias, color='blue', linestyle='-.',
	   label=r'$Bias^2$')
  ax3.plot(degree, var, color='orange', linestyle='--', label='Var',
	   alpha=0.9)
  ax3.set_ylim(0, min(50, max(mse)))
  ax3.axhline(y=1, color='grey', linestyle='--')
  min_mse_ind = mse.index(min(mse))
  ax3.axvline(x=degree[min_mse_ind], color='grey', linestyle=':')
  ax3.set_xlabel('Flexibility')
  ax3.legend()

  fig.tight_layout()
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig12
#+CAPTION: Squared bias (blue curve), variance (orange curve), $Var(\epsilon)$ (dashed line), and test MSE (red curve) for the three data sets in figures [[fig:statLearnFig9]] - [[fig:statLearnFig11]].  The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.
#+RESULTS: fig2_12plot
[[file:figures/fig2_12.png]]


Figure [[fig:statLearnFig13]] provides an example using a simulated data set in
two-dimensional space consisting of predictors $X_1$ and $X_2$.  

#+NAME: fig2_13plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_13.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap2/')
  import knnBoundry

  fig = plt.figure()
  ax = fig.add_subplot(111)
  knnBoundry.plotClassify()

  fig.tight_layout()
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig13
#+CAPTION: A simulated data set consisting of 200 observations in two groups, indicated in blue and orange.  The dashed line represents the Bayes decision boundary.  The orange background grid indicates the region in which a test observation will be assigned to the orange class, and blue background grid indicates the region in which a test observation will be assigned to the blue class. 
#+RESULTS: fig2_13plot
[[file:figures/fig2_13.png]]


Figure [[fig:statLearnFig15]] displays the KNN decision boundary, using $K=10$, when
applied to the simulated data set from figure [[fig:statLearnFig13]].  Even though
the true distribution is not known by the KNN classifier, the KNN decision
making boundary is very close to that of the Bayes classifier.  

#+NAME: fig2_15plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_15.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap2/')
  import knnBoundry

  fig = plt.figure()
  ax = fig.add_subplot(111)
  knnBoundry.plotClassify(KNN=True, region='KNN', plot_title=True)

  fig.tight_layout()
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig15
#+CAPTION: The firm line indicates the KNN decision boundary on the data from figure [[fig:statLearnFig13]], using $K = 10$. The Bayes decision boundary is shown as a dashed line.  The KNN and Bayes decision boundaries are very similar.   
#+RESULTS: fig2_15plot
[[file:figures/fig2_15.png]]


#+NAME: fig2_16plot
#+BEGIN_SRC python :exports results :results file :var fname="figures/fig2_16.png"
  import matplotlib
  matplotlib.use('Agg')
  import matplotlib.pyplot as plt
  import sys
  sys.path.append('./code/chap2/')
  import knnBoundry

  fig = plt.figure()
  ax1 = fig.add_subplot(121)
  knnBoundry.plotClassify(KNN=True, region='KNN', k_neighbors=1, plot_title=True)

  ax2 = fig.add_subplot(122)
  knnBoundry.plotClassify(KNN=True, region='KNN', k_neighbors=100,
			  plot_title=True)

  fig.tight_layout()
  plt.savefig(fname)
  return fname
#+END_SRC

#+NAME: fig:statLearnFig16
#+CAPTION: A comparison of the KNN decision boundaries (solid curves) obtained using $K=1$ and $K=100$ on the data from figure [[fig:statLearnFig13]].  With $K=1$, the decision boundary is overly flexible, while with $K=100$ it is not sufficiently flexible.  The Bayes decision boundary is shown as dashed line.
#+RESULTS: fig2_16plot
[[file:figures/fig2_16.png]]
